kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, width = "350px") %>%
column_spec(2, bold = FALSE, width = "150px") %>%
row_spec(4, bold = TRUE, color = "black", background = "#d4edda") %>%  # Verde claro
row_spec(5, bold = TRUE, color = "black", background = "#f8d7da") %>%  # Rojo claro
kable_classic(full_width = FALSE, html_font = "Arial") %>%
pack_rows("Resumen general", 1, 3, label_row_css = "background-color: #f8f9fa; font-weight: bold;") %>%
pack_rows("Películas destacadas", 4, 5, label_row_css = "background-color: #e9ecef; font-weight: bold;")
metricas_df %>%
kbl(caption = "<center><b style='font-size: 16px;'>MÉTRICAS DESCRIPTIVAS DEL ANÁLISIS</b></center>",
escape = FALSE,
col.names = NULL) %>%  # Elimina completamente los encabezados
kable_styling(bootstrap_options = c("striped", "hover"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, width = "350px") %>%
column_spec(2, width = "150px") %>%
row_spec(4, background = "#d4edda") %>%
row_spec(5, background = "#f8d7da") %>%
kable_minimal(full_width = FALSE)
# Crear data frame para la tabla
# Primero crea el data frame con las descripciones correctas
metricas_df <- data.frame(
Métrica = c("Total de reseñas analizadas",
"Total de películas",
"Ratio positivo/negativo",
"Película con mayor polaridad positiva",
"Película con mayor polaridad negativa"),
Valor = unlist(metricas_descriptivas)
)
metricas_df
metricas_df <- metricas_df[,-1]
metricas_df
metricas_df <- data.frame(
Métrica = c("Total de reseñas analizadas",
"Total de películas",
"Ratio positivo/negativo",
"Película con mayor polaridad positiva",
"Película con mayor polaridad negativa"),
Valor = unlist(metricas_descriptivas)
)
metricas_df
View(metricas_df)
metricas_descriptivas <- list(
total_resenas = nrow(polaridad_resenas),
total_peliculas = nrow(agregado_pelicula),
ratio_positivo_negativo = round(sum(polaridad_resenas$sentiment > 0) /
sum(polaridad_resenas$sentiment < 0), 2),
pelicula_mas_positiva = paste0(agregado_pelicula$film_name[which.max(agregado_pelicula$polaridad_total_pelicula)],
" (", max(agregado_pelicula$polaridad_total_pelicula), ")"),
pelicula_mas_negativa = paste0(agregado_pelicula$film_name[which.min(agregado_pelicula$polaridad_total_pelicula)],
" (", min(agregado_pelicula$polaridad_total_pelicula), ")")
)
# Primero crea el data frame con las descripciones correctas
metricas_df <- data.frame(
Métrica = c("Total de reseñas analizadas",
"Total de películas",
"Ratio positivo/negativo",
"Película con mayor polaridad positiva",
"Película con mayor polaridad negativa"),
Valor = unlist(metricas_descriptivas)
)
metricas_df
metricas_clean <- data.frame(Valor = metricas_df$Valor)
rownames(metricas_clean) <- metricas_df$Métrica
metricas_clean
# Tabla limpia y mejorada
metricas_clean %>%
kbl(caption = "<div style='text-align: center; font-weight: bold; font-size: 18px; margin-bottom: 10px;'>MÉTRICAS DESCRIPTIVAS DEL ANÁLISIS</div>",
escape = FALSE) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE,
position = "center") %>%
column_spec(1, bold = TRUE, width = "150px") %>%
row_spec(4, bold = TRUE, color = "black", background = "#d4edda") %>%  # Verde claro
row_spec(5, bold = TRUE, color = "black", background = "#f8d7da") %>%  # Rojo claro
kable_classic(full_width = FALSE, html_font = "Arial")
predicciones_lexico <- polaridad_resenas %>%
mutate(prediccion_lexico = ifelse(sentiment > 0, "positivo", "negativo")) %>%
select(review_title, prediccion_lexico)
comparativa_final <- datos_supervisado %>%
left_join(predicciones_lexico, by = c("film_name" = "review_title")) %>%
filter(!is.na(prediccion_lexico))
metricas_lexico <- calcular_metricas(comparativa_final$clase,
comparativa_final$prediccion_lexico)
comparativa_completa <- rbind(
data.frame(Enfoque = "Léxico", metricas_lexico),
data.frame(Enfoque = "Naïve Bayes", metricas_nb),
data.frame(Enfoque = "Regresión Logística", metricas_lr)
)
print("COMPARATIVA FINAL ENTRE ENFOQUES:")
print(comparativa_completa)
# Visualizar comparativa
ggplot(comparativa_completa, aes(x = Enfoque, y = Accuracy, fill = Enfoque)) +
geom_bar(stat = "identity", alpha = 0.8) +
labs(title = "Comparativa de Accuracy entre Diferentes Enfoques",
x = "Método de Análisis",
y = "Accuracy") +
theme_minimal() +
ylim(0, 1)
predicciones_nb <- predict(modelo_nb, datos_prueba)
predicciones_lr <- predict(modelo_lr, datos_prueba, type = "response")
predicciones_lr <- ifelse(predicciones_lr > 0.5, "positivo", "negativo")
predicciones_lexico <- polaridad_resenas %>%
mutate(prediccion_lexico = ifelse(sentiment > 0, "positivo", "negativo")) %>%
select(review_title, prediccion_lexico)
calcular_metricas <- function(reales, predicciones, enfoque) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
data.frame(
Enfoque = enfoque,
Accuracy = confusion_matrix$overall["Accuracy"],
Precision = confusion_matrix$byClass["Pos Pred Value"],
Recall = confusion_matrix$byClass["Sensitivity"],
F1 = confusion_matrix$byClass["F1"]
)
}
metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb, "Naïve Bayes")
metricas_lr <- calcular_metricas(clase_prueba, predicciones_lr, "Regresión Logística")
comparativa_final <- datos_supervisado %>%
left_join(predicciones_lexico, by = c("review_title" = "review_title")) %>%
filter(!is.na(prediccion_lexico))
View(datos_supervisado)
View(predicciones_lexico)
comparativa_final <- datos_supervisado %>%
left_join(predicciones_lexico, by = c("review_text" = "review_title")) %>%
filter(!is.na(prediccion_lexico))
metricas_lexico <- calcular_metricas(comparativa_final$clase,
comparativa_final$prediccion_lexico, "Léxico")
comparativa_completa <- rbind(metricas_lexico, metricas_nb, metricas_lr)
comparativa_larga <- comparativa_completa %>%
pivot_longer(cols = c(Accuracy, Precision, Recall, F1),
names_to = "Metrica", values_to = "Valor")
# Gráfico de comparación
ggplot(comparativa_larga, aes(x = Enfoque, y = Valor, fill = Enfoque)) +
geom_col(alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)), vjust = -0.5, size = 3) +
facet_wrap(~Metrica, scales = "free_y") +
labs(title = "Comparación de Métricas por Enfoque de Clasificación",
x = "Enfoque", y = "Valor") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none") +
scale_fill_brewer(palette = "Set2")
calcular_metricas_completas <- function(reales, predicciones, enfoque) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
data.frame(
Enfoque = enfoque,
Accuracy = confusion_matrix$overall["Accuracy"],
Recall = confusion_matrix$byClass["Sensitivity"],
F1 = confusion_matrix$byClass["F1"],
Kappa = confusion_matrix$overall["Kappa"]  # Índice Kappa de Cohen
)
}
# Calcular métricas para todos los enfoques
metricas_nb <- calcular_metricas_completas(clase_prueba, predicciones_nb, "Naïve Bayes")
metricas_lr <- calcular_metricas_completas(clase_prueba, predicciones_lr, "Regresión Logística")
metricas_lexico <- calcular_metricas_completas(comparativa_final$clase,
comparativa_final$prediccion_lexico, "Léxico")
comparativa_completa
# Preparar datos para gráfico
comparativa_larga <- comparativa_completa %>%
pivot_longer(cols = c(Accuracy, Recall, F1, Kappa),
names_to = "Metrica", values_to = "Valor")
# Función mejorada que maneja el caso de un solo nivel
calcular_metricas_completas <- function(reales, predicciones, enfoque) {
# Convertir a factores asegurando que tengan los mismos niveles
niveles <- unique(c(levels(factor(reales)), levels(factor(predicciones))))
reales_factor <- factor(reales, levels = niveles)
predicciones_factor <- factor(predicciones, levels = niveles)
# Si solo hay un nivel, agregar el nivel faltante artificialmente
if(length(niveles) == 1) {
niveles <- c(niveles, setdiff(c("positivo", "negativo"), niveles))
reales_factor <- factor(reales, levels = niveles)
predicciones_factor <- factor(predicciones, levels = niveles)
}
confusion_matrix <- confusionMatrix(predicciones_factor, reales_factor)
data.frame(
Enfoque = enfoque,
Accuracy = confusion_matrix$overall["Accuracy"],
Recall = confusion_matrix$byClass["Sensitivity"],
F1 = confusion_matrix$byClass["F1"],
Kappa = confusion_matrix$overall["Kappa"]
)
}
# Recalcular todas las métricas
metricas_nb <- calcular_metricas_completas(clase_prueba, predicciones_nb, "Naïve Bayes")
metricas_lr <- calcular_metricas_completas(clase_prueba, predicciones_lr, "Regresión Logística")
metricas_lexico <- calcular_metricas_completas(comparativa_final$clase,
comparativa_final$prediccion_lexico, "Léxico")
# Tabla comparativa final
comparativa_completa <- rbind(metricas_lexico, metricas_nb, metricas_lr)
# Verificar que ahora sí existe la columna Kappa
print(str(comparativa_completa))
# Función que maneja explícitamente el caso de predicciones con una sola clase
calcular_metricas_completas <- function(reales, predicciones, enfoque) {
# Verificar si hay más de una clase en las predicciones
if(length(unique(predicciones)) == 1) {
# Si solo hay una clase, calcular métricas manualmente
accuracy <- mean(predicciones == reales)
recall <- ifelse(unique(predicciones) == "positivo",
sum(predicciones == "positivo" & reales == "positivo") / sum(reales == "positivo"),
NA)
f1 <- NA
kappa <- 0  # Kappa = 0 cuando siempre predice lo mismo
return(data.frame(Enfoque = enfoque, Accuracy = accuracy, Recall = recall,
F1 = f1, Kappa = kappa))
}
# Si hay múltiples clases, calcular normalmente
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
data.frame(
Enfoque = enfoque,
Accuracy = confusion_matrix$overall["Accuracy"],
Recall = confusion_matrix$byClass["Sensitivity"],
F1 = confusion_matrix$byClass["F1"],
Kappa = confusion_matrix$overall["Kappa"]
)
}
table(comparativa_final$prediccion_lexico)
# Verificar el balance de clases real
table(comparativa_final$clase)
View(comparativa_final)
calcular_metricas_completas <- function(reales, predicciones, enfoque) {
# Verificar si hay más de una clase en las predicciones
if(length(unique(predicciones)) == 1) {
# Si solo predice una clase
clase_predicha <- unique(predicciones)
# Matriz de confusión manual
tp <- sum(predicciones == "positivo" & reales == "positivo")
tn <- sum(predicciones == "negativo" & reales == "negativo")
fp <- sum(predicciones == "positivo" & reales == "negativo")
fn <- sum(predicciones == "negativo" & reales == "positivo")
accuracy <- (tp + tn) / length(reales)
precision <- ifelse(clase_predicha == "positivo", tp / (tp + fp), tn / (tn + fn))
recall <- ifelse(clase_predicha == "positivo", tp / (tp + fn), tn / (tn + fp))
f1 <- ifelse(!is.na(precision) & !is.na(recall) & (precision + recall > 0),
2 * (precision * recall) / (precision + recall), NA)
kappa <- 0  # Kappa = 0 cuando siempre predice lo mismo
return(data.frame(Enfoque = enfoque, Accuracy = accuracy, Recall = recall,
F1 = f1, Kappa = kappa))
}
# Si hay múltiples clases, calcular normalmente
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
data.frame(
Enfoque = enfoque,
Accuracy = confusion_matrix$overall["Accuracy"],
Recall = confusion_matrix$byClass["Sensitivity"],
F1 = confusion_matrix$byClass["F1"],
Kappa = confusion_matrix$overall["Kappa"]
)
}
comparativa_completa <- rbind(metricas_lexico, metricas_nb, metricas_lr)
# Verificar que ahora sí existe la columna Kappa
print((comparativa_completa))
predicciones_lexico <- polaridad_resenas %>%
mutate(prediccion_lexico = ifelse(sentiment > 0, "positivo", "negativo")) %>%
select(review_title, prediccion_lexico)
# Unir con datos reales (asumiendo que tenemos ratings como proxy de verdad real)
comparativa_final <- datos_supervisado %>%
left_join(predicciones_lexico, by = c("film_name" = "review_title")) %>%
filter(!is.na(prediccion_lexico))
metricas_lexico <- calcular_metricas(comparativa_final$clase,
comparativa_final$prediccion_lexico)
# Función para calcular métricas (sin Precision, con Kappa)
calcular_metricas <- function(reales, predicciones) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
accuracy <- confusion_matrix$overall["Accuracy"]
recall <- confusion_matrix$byClass["Sensitivity"]
f1 <- confusion_matrix$byClass["F1"]
kappa <- confusion_matrix$overall["Kappa"]  # Reemplazamos Precision por Kappa
return(data.frame(Accuracy = accuracy,
Recall = recall,
F1 = f1,
Kappa = kappa))  # Kappa en lugar de Precision
}
# Calcular métricas para cada enfoque
metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb)
metricas_lr <- calcular_metricas(clase_prueba, predicciones_lr)
metricas_lexico <- calcular_metricas(comparativa_final$clase, comparativa_final$prediccion_lexico)
# Tabla comparativa final
comparativa_completa <- rbind(
data.frame(Enfoque = "Léxico", metricas_lexico),
data.frame(Enfoque = "Naïve Bayes", metricas_nb),
data.frame(Enfoque = "Regresión Logística", metricas_lr)
)
print("COMPARATIVA FINAL ENTRE ENFOQUES:")
print(comparativa_completa)
View(comparativa_final)
View(comparativa_completa)
comparativa_completa <- comparativa_completa %>%
tibble::column_to_rownames(var = "Enfoque")
# Eliminar la columna "Enfoque" que ahora es redundante
comparativa_completa$Enfoque <- NULL
comparativa_completa <- rbind(
data.frame(Enfoque = "Léxico", metricas_lexico),
data.frame(Enfoque = "Naïve Bayes", metricas_nb),
data.frame(Enfoque = "Regresión Logística", metricas_lr)
)
print("COMPARATIVA FINAL ENTRE ENFOQUES:")
print(comparativa_completa)
rownames(comparativa_completa) <- comparativa_completa$Enfoque
# Eliminar la columna "Enfoque" que ahora es redundante
comparativa_completa$Enfoque <- NULL
# Mostrar la tabla con Enfoque como índice
print(comparativa_completa)
comparativa_completa %>%
kbl(caption = "Comparativa de Métricas por Enfoque",
digits = 3) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
row_spec(which.max(comparativa_completa$Kappa),
background = "#e6f7ff", bold = TRUE)
comparativa_completa %>%
kbl(caption = "",
digits = 3) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
row_spec(which.max(comparativa_completa$Kappa),
background = "#e6f7ff", bold = TRUE)
comparativa_grafico <- comparativa_completa %>%
tibble::rownames_to_column(var = "Enfoque") %>%
pivot_longer(cols = -Enfoque,
names_to = "Metrica",
values_to = "Valor")
# Crear el gráfico comparativo
ggplot(comparativa_grafico, aes(x = Metrica, y = Valor, fill = Enfoque)) +
geom_col(position = "dodge", alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3) +
labs(title = "Comparativa de Métricas por Enfoque",
x = "Métrica",
y = "Valor",
fill = "Enfoque") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
legend.position = "bottom") +
scale_fill_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c")) +  # Colores distintos
scale_y_continuous(limits = c(0, 1)) +  # Para métricas normalizadas
facet_wrap(~ Metrica, scales = "free_y", nrow = 1)  # Una fila con 4 columnas
Versión alternativa más compacta:
# Gráfico de barras agrupadas
ggplot(comparativa_grafico, aes(x = Enfoque, y = Valor, fill = Metrica)) +
geom_col(position = "dodge", alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3) +
labs(title = "Comparativa de Métricas por Enfoque",
x = "Enfoque",
y = "Valor",
fill = "Métrica") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5)) +
scale_fill_brewer(palette = "Set1") +
scale_y_continuous(limits = c(0, 1))
# Preparar datos para el gráfico (usando comparativa_completa con rownames)
comparativa_grafico <- comparativa_completa %>%
tibble::rownames_to_column(var = "Enfoque") %>%
pivot_longer(cols = -Enfoque,
names_to = "Metrica",
values_to = "Valor")
# Ordenar las métricas en el orden deseado
comparativa_grafico$Metrica <- factor(comparativa_grafico$Metrica,
levels = c("Accuracy", "Recall", "F1", "Kappa"))
# Crear el gráfico de barras agrupadas (como en la imagen)
ggplot(comparativa_grafico, aes(x = Metrica, y = Valor, fill = Enfoque)) +
geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)),
position = position_dodge(width = 0.8),
vjust = -0.5,
size = 3.5,
fontface = "bold") +
labs(title = "Comparación de Métricas por Enfoque de Clasificación",
x = "Métrica",
y = "Valor",
fill = "Enfoque") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(face = "bold", size = 10),
legend.position = "bottom",
legend.title = element_text(face = "bold")
) +
scale_fill_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c")) +  # Azul, naranja, verde
scale_y_continuous(limits = c(0, 1),
breaks = seq(0, 1, 0.25),
labels = scales::percent_format(accuracy = 1)) +
scale_x_discrete(labels = c("Accuracy", "Recall", "F1", "Kappa"))
calcular_metricas <- function(reales, predicciones) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
accuracy <- confusion_matrix$overall["Accuracy"]
recall <- confusion_matrix$byClass["Sensitivity"]
f1 <- confusion_matrix$byClass["F1"]
kappa <- confusion_matrix$overall["Kappa"]
return(data.frame(Accuracy = accuracy,
Recall = recall,
F1 = f1,
Kappa = kappa))
}
metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb)
metricas_lr <- calcular_metricas(clase_prueba, predicciones_lr)
metricas_lexico <- calcular_metricas(comparativa_final$clase, comparativa_final$prediccion_lexico)
comparativa_completa <- rbind(
data.frame(Enfoque = "Léxico", metricas_lexico),
data.frame(Enfoque = "Naïve Bayes", metricas_nb),
data.frame(Enfoque = "Regresión Logística", metricas_lr))
rownames(comparativa_completa) <- comparativa_completa$Enfoque;comparativa_completa$Enfoque <- NULL
comparativa_completa %>%
kbl(caption = "",
digits = 3) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
row_spec(which.max(comparativa_completa$Kappa),
background = "#e6f7ff", bold = TRUE)
comparativa_grafico <- comparativa_completa %>%
tibble::rownames_to_column(var = "Enfoque") %>%
pivot_longer(cols = -Enfoque,
names_to = "Metrica",
values_to = "Valor")
comparativa_grafico$Metrica <- factor(comparativa_grafico$Metrica,
levels = c("Accuracy", "Recall", "F1", "Kappa"))
ggplot(comparativa_grafico, aes(x = Metrica, y = Valor, fill = Enfoque)) +
geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)),
position = position_dodge(width = 0.8),
vjust = -0.5,
size = 3.5,
fontface = "bold") +
labs(title = "Comparación de Métricas por Enfoque de Clasificación",
x = "Métrica",
y = "Valor",
fill = "Enfoque") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(face = "bold", size = 10),
legend.position = "bottom",
legend.title = element_text(face = "bold")
) +
scale_fill_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c")) +
scale_y_continuous(limits = c(0, 1),
breaks = seq(0, 1, 0.25),
labels = scales::percent_format(accuracy = 1)) +
scale_x_discrete(labels = c("Accuracy", "Recall", "F1", "Kappa"))
calcular_metricas <- function(reales, predicciones) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
accuracy <- confusion_matrix$overall["Accuracy"]
f1 <- confusion_matrix$byClass["F1"]
kappa <- confusion_matrix$overall["Kappa"]
return(data.frame(Accuracy = accuracy,
Recall = recall,
F1 = f1,
Kappa = kappa))
}
metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb)
calcular_metricas <- function(reales, predicciones) {
confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
accuracy <- confusion_matrix$overall["Accuracy"]
f1 <- confusion_matrix$byClass["F1"]
kappa <- confusion_matrix$overall["Kappa"]
return(data.frame(Accuracy = accuracy,
F1 = f1,
Kappa = kappa))
}
metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb)
metricas_lr <- calcular_metricas(clase_prueba, predicciones_lr)
metricas_lexico <- calcular_metricas(comparativa_final$clase, comparativa_final$prediccion_lexico)
comparativa_completa <- rbind(
data.frame(Enfoque = "Léxico", metricas_lexico),
data.frame(Enfoque = "Naïve Bayes", metricas_nb),
data.frame(Enfoque = "Regresión Logística", metricas_lr))
rownames(comparativa_completa) <- comparativa_completa$Enfoque;comparativa_completa$Enfoque <- NULL
comparativa_completa %>%
kbl(caption = "",
digits = 3) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
row_spec(which.max(comparativa_completa$Kappa),
background = "#e6f7ff", bold = TRUE)
comparativa_grafico <- comparativa_completa %>%
tibble::rownames_to_column(var = "Enfoque") %>%
pivot_longer(cols = -Enfoque,
names_to = "Metrica",
values_to = "Valor")
comparativa_grafico$Metrica <- factor(comparativa_grafico$Metrica,
levels = c("Accuracy", "F1", "Kappa"))
ggplot(comparativa_grafico, aes(x = Metrica, y = Valor, fill = Enfoque)) +
geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.8) +
geom_text(aes(label = round(Valor, 3)),
position = position_dodge(width = 0.8),
vjust = -0.5,
size = 3.5,
fontface = "bold") +
labs(title = "Comparación de Métricas por Enfoque de Clasificación",
x = "Métrica",
y = "Valor",
fill = "Enfoque") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(face = "bold", size = 10),
legend.position = "bottom",
legend.title = element_text(face = "bold")
) +
scale_fill_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c")) +
scale_y_continuous(limits = c(0, 1),
breaks = seq(0, 1, 0.25),
labels = scales::percent_format(accuracy = 1)) +
scale_x_discrete(labels = c("Accuracy", "F1", "Kappa"))
View(datos_supervisado)
