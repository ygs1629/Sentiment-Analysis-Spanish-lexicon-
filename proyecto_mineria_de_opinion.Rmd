---
title: "Minería de opinión con léxico en castellano"
author: "Juan Luis German Saura"
date: "Sept-2025"
output:
  prettydoc::html_pretty:
    toc: True
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```

\

## Introducción

\
\
Los datos no estructurados representan una dimensión altamente valiosa en el panorama actual de la ciencia de datos. Al carecer de una estructura u organización predefinida, requieren ser etiquetados dentro de un marco conceptual específico para extraer información significativa de esta aparente desorganización. Independientemente de la metodología elegida, siempre existen limitaciones de las que es crucial ser consciente (interpretación subjetiva del lenguaje), aunque esto no disminuye el enorme potencial que surge al poder capturar y analizar la representación de la realidad a través del lenguaje natural.\
\
Por ello, nace la necesidad de abordar este desafío, inspirándose en una tarea académica del grado universitario cursado (BIA). Su objetivo principal es aplicar técnicas de minería de opinión en español para analizar reseñas de películas, explorando la relación entre la polaridad de los comentarios y factores como el género cinematográfico y la puntuación promedio.\
\
Este proyecto analiza reseñas de películas (datos ocultos por razones de privacidad) para evaluar su polaridad de sentimiento mediante un enfoque léxico y modelos supervisados. Se emplean técnicas de procesamiento de texto (tokenización, limpieza, matriz término-documento) y modelos de clasificación como Naïve Bayes y Regresión Logística. La evaluación se realiza mediante métricas estándar de clasificación: accuracy, precisión, recall y F1. Se utilizan herramientas de R, tidyverse, tm, wordcloud y ggplot2, desarrollando habilidades en análisis de texto, preprocesamiento, modelado supervisado y visualización de datos.\
\
\

## Preparación de datos para modelo léxico

\
En primer lugar, se hace la carga de las librerías y el conjunto de datos. Seguidamente, se carga el código que contiene el filtrado de léxico, para poder disponer de un diccionario que permita calcular la polaridad del texto en la lengua española. Para evitar interpretaciones erradas, se hace una serie de transformaciones que aseguran que las variables derivadas sean comparables y que el ruido del lenguaje natural no afecte los resultados de clasificación.\
\

```{r}
library(tidyverse)
library(tidytext)
library(rio)
library(ggplot2)
library(dplyr)
library(syuzhet)
library(tidyverse)
library(tm)
library(wordcloud)
library(caret)
library(e1071)  
library(glmnet) 
library(kableExtra)
library(DT)
```

```{r,echo=FALSE}
load("C:/Users/Yannis/Documents/muestra_reviews_filmaffinity.RData")
```

```{r,echo=TRUE, results='hide'}
nrc_sp <- get_sentiment_dictionary(dictionary = "nrc",language = "spanish") %>% 
  filter(sentiment %in% c("positive","negative")) %>% distinct()
nrc_sp$word <- tolower(nrc_sp$word)


nrc_sp %>% group_by(word) %>% summarise(n=n()) %>% arrange(desc(n)) %>% 
  tibble() %>% filter(n>1)
nrc_sp %>% filter(word=="encantado") 
lista_duplicados <- nrc_sp %>% count(word,sort = TRUE) %>% filter(n>1) %>% pull(word)
lista_duplicados
nrc_sp <- nrc_sp %>% filter(!word %in% lista_duplicados)
```

\
Cabe resaltar que, en el análisis léxico directo, el conteo palabra a palabra permite una mayor interpretabilidad y fidelidad en la medición de polaridad, por lo que la eliminación de términos poco frecuentes podría ocasionar una pérdida de información significativa, siendo menos recomendable en este contexto.\
\
De esta forma, ya se encuentran los detalles preparados para poder analizar la polaridad en las reseñas del público sobre películas producidas en el cine español.\
\

## Análisis polaridad

\
\
Para poder desarrollar el análisis de polaridad sobre las reseñas individuales y luego sobre las películas, esta acción compleja se divide en unas partes claramente delimitadas.\
\
En primer lugar, se descompone el corpus de documentos en cada uno de los tokens que lo compone, para posteriormente poder hacer una comparación mediante el diccionario de sentimientos NCR para clasificar al token correspondiente según si su naturaleza es positiva o negativa.\
\

```{r}
tokens <- datos %>%
  unnest_tokens(word, review_text) %>%
  inner_join(nrc_sp, by = "word")
```

\
En segundo lugar, se elaboran los cálculos pertinentes para el análisis individual por reseña y el total por película. Para ello, se hace una agrupación según el título de la película y el título de la reseña y seguido de ello, se calcula el total de palabras positivas, negativas, el número de tokens totales y la polaridad de (diferencia entre las positivas y negativas).\

```{r}
polaridad_resenas <- tokens %>%
  group_by(film_name, review_title) %>%
  summarise(
    positive = sum(sentiment == "positive", na.rm = TRUE),  
    negative = sum(sentiment == "negative", na.rm = TRUE),  
    sentiment = positive - negative,                       
    total_token = n(),                                 
    .groups = 'drop')

head(polaridad_resenas)
```

\
Se puede ver que se ha conseguido con éxito lo que se pretendía generar, ya que se ha permitido evaluar la polaridad individual de cada resena antes de agrupar los resultados para cada película.\
\
Por último, se agregan las reseñas por película y se intenta conseguir la polaridad total por película, la polaridad media por reseña y por token, así como el total de reseñas y token por película.\

```{r}
agregado_pelicula <- polaridad_resenas %>%
  group_by(film_name) %>%
  summarise(
    polaridad_total_pelicula = sum(sentiment, na.rm = TRUE),  
    polaridad_media_resena = mean(sentiment, na.rm = TRUE),   
    total_resenas = n(),                                      
    total_tokens_pelicula = sum(total_token, na.rm = TRUE),  
    polaridad_media_token = polaridad_total_pelicula / total_tokens_pelicula,  
    .groups = 'drop')

head(agregado_pelicula)
```

\
Con estos valores, se ha permitido la evaluación sobre cómo varía el tono general ya sea positivo o negativo para cada película en su conjunto, facilitando la comparación entre largometrajes.\
\

## Conjunto de datos con medidas de polaridad y metavariables

\
Para poder llegar a hacer una examinación más rica y profunda, se opta por fusionar las metavariables iniciales que se contenían en el dataframe incial junto con las medidas de polaridad recientemente generadas.\

```{r}
df_final <- agregado_pelicula %>%
  left_join(select(datos, film_name, gender, film_avg_rate), by = "film_name")%>% 
  distinct()

head(df_final)
```

\
Con una inspección inicial se observan variaciones en la polaridad entre géneros cinematográficos, aunque con la muestra reducida los patrones específicos pueden diferir de los identificados previamente. Asimismo, la relación entre la polaridad total y la valoración del público merece un análisis más detallado para identificar posibles discrepancias.\
\
Asimismo, se tendría que indagar más sobre la polaridad total que tiene cada película y la valoración que otorga el público. Cabe la posibilidad de que existan posibles discrepancias entre la percepción del público y las reseñas críticas.\
\

## EDA

\
En esta sección se realiza un análisis exploratorio de los datos con el objetivo de identificar patrones y relaciones antes de entrenar modelos supervisados. Se combinan métricas descriptivas, análisis de correlación y visualizaciones léxicas para comprender cómo varía la polaridad de las reseñas según la puntuación promedio y el género cinematográfico.\
\
Como punto de partida, se calculan métricas básicas que ofrecen un contexto general: número total de reseñas y películas, ratio de reseñas positivas frente a negativas y las películas con polaridad extrema. Esta primera aproximación muestra una clara mayoría de reseñas positivas, aunque con una dispersión considerable entre títulos, lo que sugiere que la distribución del sentimiento podría no ser uniforme.\
\

```{r}
metricas_descriptivas <- list(
  total_resenas = nrow(polaridad_resenas),
  total_peliculas = nrow(agregado_pelicula),
  ratio_positivo_negativo = round(sum(polaridad_resenas$sentiment > 0) / sum(polaridad_resenas$sentiment < 0), 2),
  pelicula_mas_positiva = paste0(agregado_pelicula$film_name[which.max(agregado_pelicula$polaridad_total_pelicula)],
                                 " (", max(agregado_pelicula$polaridad_total_pelicula), ")"),
  pelicula_mas_negativa = paste0(agregado_pelicula$film_name[which.min(agregado_pelicula$polaridad_total_pelicula)],
                                 " (", min(agregado_pelicula$polaridad_total_pelicula), ")"))

metricas_df <- data.frame(
  Metrica = c("Total de reseñas analizadas", 
              "Total de películas",
              "Ratio positivo/negativo",
              "Película con mayor polaridad positiva",
              "Película con mayor polaridad negativa"),
  Valor = unlist(metricas_descriptivas))

metricas_clean <- data.frame(Valor = metricas_df$Valor)
rownames(metricas_clean) <- metricas_df$Metrica

metricas_clean %>%
  kbl(caption = "<div style='text-align: center; font-weight: bold; font-size: 18px; margin-bottom: 10px;'>Métricas descriptivas del análisis</div>",
      escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE,
                position = "center") %>%
  column_spec(1, bold = TRUE, width = "150px") %>%
  row_spec(4, bold = TRUE, color = "black", background = "#d4edda") %>% 
  row_spec(5, bold = TRUE, color = "black", background = "#f8d7da") %>%  
  kable_classic(full_width = FALSE, html_font = "Arial")
```

\
Esta primera aproximación muestra una clara mayoría de reseñas positivas, aunque con una dispersión considerable entre títulos, lo que sugiere que la distribución del sentimiento podría no ser uniforme.\
\
A continuación, se evalúa de manera rigurosa la relación entre la puntuación media de las películas y la polaridad media de las reseñas mediante el cálculo de correlaciones globales y segmentadas por género.\

```{r}
df_final <- df_final %>% mutate(film_avg_rate = as.numeric(gsub(",", ".", film_avg_rate)))

cor_global <- cor(df_final$film_avg_rate, 
                  df_final$polaridad_media_resena, use = "complete.obs")

cor_segmented <- df_final %>%
  group_by(gender) %>%
  summarise(correlation = cor(film_avg_rate, polaridad_media_resena,
                              use = "complete.obs")) %>%
  mutate(cor_label = paste(gender, ": r =", round(correlation, 2)))
```

\
Los resultados muestran una correlación global negativa débil (-0.28). Esto sugiere que, en general, a mayor puntuación de la película tiende a haber reseñas ligeramente menos positivas, pero la relación no es lineal ni uniforme. La influencia de otros factores, como el género es crucial para interpretar correctamente la relación entre la calidad de la película y el tono de las críticas.\

```{r}
ggplot(df_final, aes(x = film_avg_rate, y = polaridad_media_resena, color = gender)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, aes(group = gender)) +
  labs(title = paste("Correlación global:", round(cor_global, 2)),
       x = "Puntuación media de la película",
       y = "Polaridad media de las resenas") +
  theme_minimal() +
  scale_color_discrete(name = "Género")
```

\
Otra perspectiva a analizar es la distribución de la polaridad según el género, que aportarán más contexto al resultado encontrado en la correlación.\

```{r}
ggplot(polaridad_resenas, aes(x = sentiment, fill = ..x.. > 0)) +
  geom_histogram(bins = 30, alpha = 0.7) +
  facet_wrap(~ datos$gender[match(polaridad_resenas$film_name, datos$film_name)], 
             scales = "free_y") +
  labs(title = "Distribución de polaridad por género cinematográfico",
       x = "Polaridad de reseña", 
       y = "Frecuencia") +
  scale_fill_manual(values = c("red", "green"), 
                    name = "Sentimiento",
                    labels = c("Negativo", "Positivo")) +
  theme_minimal()
```

\
Se confirma que la mayoría de las reseñas son positivas en todos los géneros. Sin embargo, la distribución de la polaridad varía notablemente entre ellos. Esto indica que el género actúa como un filtro emocional que condiciona el tono de las críticas de forma independiente a la puntuación objetiva de la película.\
\
Para indagar más en el análisis, se opta por graficar la frecuencia de la distribución de clases de sentimiento dentro del dataframe.\

```{r}
distribucion_clases <- data.frame(
  Clase = c("Muy Negativo", "Negativo", "Neutral", "Positivo", "Muy Positivo"),
  Count = c(
    sum(polaridad_resenas$sentiment < -5),
    sum(polaridad_resenas$sentiment >= -5 & polaridad_resenas$sentiment < 0),
    sum(polaridad_resenas$sentiment == 0),
    sum(polaridad_resenas$sentiment > 0 & polaridad_resenas$sentiment <= 5),
    sum(polaridad_resenas$sentiment > 5)
  )
)

ggplot(distribucion_clases, aes(x = Clase, y = Count, fill = Clase)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribución de clasificaciones de sentimiento",
       x = "Categoría de sentimiento",
       y = "Número de reseñas") +
  theme_minimal() +
  scale_fill_brewer(palette = "RdYlGn")
```

\
Con este gráfico se observa que la mayoría de reseñas tienden a ser positivas y muy positivas, aunque también existe una proporción relevante de valoraciones negativas. Sin embargo, este predominio de positividad no se traduce en una relación consistente con la puntuación media de las películas, tal como demuestran la correlación débil y las diferencias según género.\
\
Tomando en consideración lo anterior, no se puede descartar la hipótesis de que el comportamiento mayoritario de las reseñas refleja tendencias emocionales de los usuarios, pero no constituye un predictor fiable de la valoración numérica de las películas.\
\
Finalmente, para complementar los resultados numéricos, se representan las nubes de palabras más frecuentes en reseñas positivas y negativas.\
\

```{r}
palabras_por_clase <- tokens %>%
  mutate(clase_sentimiento = ifelse(sentiment == "positive", 
                                    "Positivo", "Negativo")) %>%
  group_by(clase_sentimiento, word) %>%
  summarise(frecuencia = n(), .groups = "drop") %>%
  group_by(clase_sentimiento) %>%
  slice_max(frecuencia, n = 20)

palabras_positivas <- filter(palabras_por_clase, clase_sentimiento == "Positivo")

wordcloud(words = palabras_positivas$word, 
          freq = palabras_positivas$frecuencia,
          max.words = 50,
          scale = c(3, 0.5),    
          min.freq = 1,         
          random.order = FALSE, 
          rot.per = 0.2,       
          colors = brewer.pal(9, "Greens"),
          fixed.asp = TRUE)
```

\

```{r}
palabras_negativas <- filter(palabras_por_clase, clase_sentimiento == "Negativo")

wordcloud(words = palabras_negativas$word, 
          freq = palabras_negativas$frecuencia,
          max.words = 50,
          scale = c(3,1.5),    
          min.freq = 1,         
          random.order = FALSE, 
          rot.per = 0.2,       
          colors = brewer.pal(9,"Reds"),
          fixed.asp = TRUE)
```

\
Las reseñas positivas se centran en elementos concretos de la producción como la "actuación" y el "humor", mientras que las negativas critican abiertamente fallos en el "argumento" y la "interpretación". Esta división confirma que las métricas numéricas por sí solas ocultan matices cruciales, ya que los espectadores perdonan menos los defectos narrativos que otros aspectos técnicos, lo que podría explicar la disparidad entre polaridad y la puntuación final.\
\

## Preparación y preprocesamiento de datos para modelos supervisados

\
Los modelos supervisados pueden ser una alternativa para la tarea que hasta ahora solo estaba haciendo el método léxico. Ello se debe a que estos no dependen de la valoración de puntuación que tiene asignada el diccionario usado para el método anterior, ya que estos se dedican a capturar combinaciones de palabras inadvertidas anteriormente.Adicionalmente, se ajustan mejor al contexto específico sobre el que se utilizan. Debido a esto, merecen una comparativa frente al modelo léxico por las ventajas que ofrecen por su mayor (aparente) precisión, sin olvidar que este tipo de modelos suelen ser más costoso computacionalmene en su fase de entranamiento.\
\
Para poder implementar estos modelos, se pasa a la transformación la variable de puntuación en formato numérico y se genera la variable objetivo binaria. A continuación, se realiza el habitual preprocesamiento con la correspondiente tokenización, limpieza, eliminación de palabras vacías y creación de la matriz término-documento.\
\
Opuesto al método inicial, en los modelos supervisados, la reducción de dimensionalidad resulta útil para filtrar términos escasos que generan ruido en la matriz término-documento, mejorando la eficiencia y la capacidad de generalización de los algoritmos.\
\
Estas transformaciones aseguran que los datos sean adecuados para el entrenamiento de modelos supervisados.\
\

```{r}
datos_supervisado <- datos %>%
  mutate(rating_num = as.numeric(gsub(",", ".", film_avg_rate)),
         clase = ifelse(rating_num >= 6, "positivo", "negativo")) %>%
  filter(!is.na(clase)) %>%
  select(review_text, clase, film_name, film_avg_rate) %>%
  distinct()

crear_matriz_documentos <- function(textos) {
  corpus <- Corpus(VectorSource(textos))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
  corpus <- tm_map(corpus, stripWhitespace)
  
  dtm <- DocumentTermMatrix(corpus)
  return(dtm)
}

dtm <- crear_matriz_documentos(datos_supervisado$review_text)

dtm_reducido <- removeSparseTerms(dtm, 0.995)
matriz_datos <- as.matrix(dtm_reducido)
```

\
\

## Entrenamiento de modelos supervisados

\
Se entrenan modelos de Naïve Bayes y Regresión Logística para clasificar las reseñas, utilizando una partición estratificada de los datos entre train set y test set. Este procedimiento asegura que la evaluación del modelo refleje fielmente su capacidad de generalización. La comparación entre enfoques permite identificar qué método captura de manera más consistente la relación entre el léxico empleado y la polaridad de las reseñas, aportando información sobre la robustez de las técnicas aplicadas.\

```{r}
set.seed(123)
indices_entrenamiento <- createDataPartition(datos_supervisado$clase, p = 0.8, list = FALSE)

datos_entrenamiento <- matriz_datos[indices_entrenamiento, ]
datos_prueba <- matriz_datos[-indices_entrenamiento, ]
clase_entrenamiento <- datos_supervisado$clase[indices_entrenamiento]
clase_prueba <- datos_supervisado$clase[-indices_entrenamiento]


modelo_nb <- naiveBayes(datos_entrenamiento, clase_entrenamiento)

modelo_lr <- cv.glmnet(datos_entrenamiento, 
                       as.numeric(clase_entrenamiento == "positivo"),
                       family = "binomial", alpha = 1)
```

\
\

## Evaluación y comparativa entre modelos

\
La evaluación se centra en métricas de clasificación estándar(accuracy, recall, F1 y Kappa) que proporcionan una visión integral del rendimiento de cada modelo. Esta evaluación complementa el análisis previo de correlación, mostrando cómo las relaciones léxicas detectadas se traducen en capacidad predictiva.\

```{r}
predicciones_nb <- predict(modelo_nb, datos_prueba)
predicciones_lr <- predict(modelo_lr, datos_prueba, type = "response")
predicciones_lr <- ifelse(predicciones_lr > 0.5, "positivo", "negativo")


calcular_polaridad_texto <- function(texto) {
  tokens <- data.frame(word = unlist(strsplit(tolower(texto), "\\s+")))
  sentimiento <- tokens %>%
    inner_join(nrc_sp, by = "word") %>%
    summarise(total = sum(ifelse(sentiment == "positive", 1, -1), na.rm = TRUE))
  return(ifelse(sentimiento$total > 0, "positivo", "negativo"))
}

predicciones_lexico_prueba <- sapply(datos_supervisado$review_text[-indices_entrenamiento], 
                                     calcular_polaridad_texto)
```

\
\
Tanto mediante la tabla como con el gráfico combinado, se puede comprobar que, a rasgos generales, el mejor clasificador es el Naïve Bayes. Solamente bajo algún contexto específico se podría argumentar el uso de otro método por encima de este.\
\
El Naïve Bayes demuestra un rendimiento excepcional, con una accuracy del 89.7% y un Kappa de 0.784, lo que indica una mejora significativa en sus predicciones respecto a hacerlas aleatoriamente, por lo que demuestra una capacidad sobresaliente para capturar patrones subyacentes. Su equilibrio entre precisión y exhaustividad (F1 = 0.870) confirma su robustez para esta tarea..\
\
La Regresión Logística presenta un rendimiento muy alto y casi idéntico (accuracy = 89.4%, Kappa = 0.773), mostrando una mejora excelente sobre el azar, pero ligeramente por detrás del enfoque de Naïve Bayes. Es importante destacar que su valor F1 (0.856) es muy cercano al de Naïve Bayes, lo que sugiere que ambos modelos son altamente competentes. La mínima diferencia indica que la elección entre uno y otro podría depender de consideraciones prácticas como la velocidad de entrenamiento o de la interpretabilidad.\
\
El enfoque léxico, por el contrario, evidencia serias limitaciones. Con una accuracy del 56.1% y un Kappa muy bajo (0.064), este método apenas mejora sobre el azar. Su valor F1 de 0.409 confirma su incapacidad para realizar clasificaciones útiles y precisas en este contexto. Estos resultados subrayan que la interpretación del contexto y el dominio del área requieren modelos más sofisticados que el simple análisis de léxico.\
\
Por ello, en la gran mayoría de aplicaciones prácticas la elección preferida sería el Naïve Bayes, reservando la Regresión Logística para casos donde su principal ventaja, que es la interpretabilidad, sea un factor crítico.\
\

```{r}
calcular_metricas <- function(reales, predicciones) {
  confusion_matrix <- confusionMatrix(factor(predicciones), factor(reales))
  accuracy <- confusion_matrix$overall["Accuracy"]
  f1 <- confusion_matrix$byClass["F1"]
  kappa <- confusion_matrix$overall["Kappa"]  
  
  return(data.frame(Accuracy = accuracy, 
                    F1 = f1,
                    Kappa = kappa))  
}

comparativa_final <- data.frame (clase = clase_prueba,
                    prediccion_lexico = predicciones_lexico_prueba)

metricas_nb <- calcular_metricas(clase_prueba, predicciones_nb)
metricas_lr <- calcular_metricas(clase_prueba, predicciones_lr)
metricas_lexico <- calcular_metricas(comparativa_final$clase, comparativa_final$prediccion_lexico)


comparativa_completa <- rbind(
  data.frame(Enfoque = "Léxico", metricas_lexico),
  data.frame(Enfoque = "Naïve Bayes", metricas_nb),
  data.frame(Enfoque = "Regresión Logística", metricas_lr))

rownames(comparativa_completa) <- comparativa_completa$Enfoque
comparativa_completa$Enfoque <- NULL

comparativa_completa %>%
  kbl(caption = "",
      digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(which.max(comparativa_completa$Kappa), 
           background = "#e6f7ff", bold = TRUE)
```

\

```{r}
comparativa_grafico <- comparativa_completa %>%
  tibble::rownames_to_column(var = "Enfoque") %>%
  pivot_longer(cols = -Enfoque, 
               names_to = "Metrica", 
               values_to = "Valor")
ggplot(comparativa_grafico, aes(x = Metrica, y = Valor, fill = Enfoque)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.8) +
  geom_text(aes(label = round(Valor, 3)), 
            position = position_dodge(width = 0.8), 
            vjust = ifelse(comparativa_grafico$Valor < 0, 1.5, -0.5),  
            size = 3.5,
            fontface = "bold") +
  labs(title = "Comparación de métricas por enfoque de clasificación",
       x = "Métrica",
       y = "Valor",
       fill = "Enfoque") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.title = element_text(face = "bold"),
    axis.text.x = element_text(face = "bold", size = 10),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  ) +
  scale_fill_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c")) +
  scale_y_continuous(limits = c(-0.25, 1),  # Límite inferior extendido a -0.25
                     breaks = seq(-0.25, 1, 0.25),  # Marcas cada 0.25 desde -0.25
                     labels = function(x) ifelse(x < 0, 
                                                 paste0(format(x, nsmall = 2)), 
                                                 scales::percent_format(accuracy = 1)(x))) +
  scale_x_discrete(labels = c("Accuracy", "F1", "Kappa")) +
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5)
```

\
\

## Conclusiones finales

\
Para condensar los hallazgos claves, es esencial repasar que aunque las reseñas tienden a ser positivas, la correlación global débil (-0.28) entre polaridad y puntuación indica que el sentimiento expresado en texto no predice consistentemente la valoración numérica. Añadido a esto, la relación entre polaridad y puntuación depende del género, como se observa en las distribuciones de polaridad por género, lo que refleja diferencias en cómo se verbalizan las valoraciones.\
\
Los resultados demuestran de manera contundente la superioridad de los modelos supervisados sobre el enfoque léxico para la clasificación de polaridad en reseñas cinematográficas en español. El Naïve Bayes emerge como el método preferente para aplicaciones generales (accuracy del 89.7%, F1 de 0.870), superando ligeramente a la regresión logística en todas las métricas. El Naïve Bayes demuestra una capacidad robusta para capturar patrones lingüísticos, mientras que la regresión logística sigue siendo una alternativa sólida y casi equivalente.\

A pesar de esto, cabe puntualizar que el rendimiento limitado del método léxico en este contexto (accuracy del 56.1%, Kappa de 0.064) no implica que sea inviable en otros dominios. Su valor fundamental reside en su eficiencia computacional y rapidez de implementación, ya que no requiere entrenamiento previo y puede aplicarse inmediatamente a nuevos datos. Por tanto, probar y refinar este enfoque con nuevos datos y dominios específicos sigue siendo aconsejable.